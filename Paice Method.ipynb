{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Paice method for evaluation of Stemmer\n",
    "\n",
    "## Stemming\n",
    "Stemming of a word essentially means to remove all linguistic inflections present in the word and transform it into a simple *word stem*. This is usually done to improve recall in an IR system (think search engine) and also for removing noises while constructing statistical models like *Bag of Words*. For example, the words *walking, walked,* and *walks* are all variations of the verb *walk*. In the context of a search engine, it is desirable to fetch all the documents containing these terms when the input query is *walk*.\n",
    "\n",
    "Stemming is usually done by stripping off suffixes and prefixes from words. If we are to consider a simple stemmer which strips only the suffixes *-ing, -ed,* and *-s*, then the words can be stemmed as follows:\n",
    "\n",
    "* walking -> walk\n",
    "* walked -> walk\n",
    "* walks -> walk\n",
    "* walk -> walk\n",
    "\n",
    "The above example can be a bit misleading because stemming does not need to produce a grammatically correct word. Consider the words *tasted* and *tasting*. When stemmed by stripping the suffixes described above, these words become:\n",
    "\n",
    "* tasted -> tast\n",
    "* tasting -> tast\n",
    "\n",
    "This is not necessarily incorrect. All that is required of a stemmer is that it conflate the related inflections to a single stem even if the stem is not a grammatically valid word. But consider the word *taste* which, when stemmed, remains *taste* i.e. it has different stem than *tasted* and *tasting* even though all three of them are related. This is an error. A *under-stemming* error to be precise.\n",
    "\n",
    "<img src=\"images/stem_walk.png\"></img>\n",
    "\n",
    "## Evaluating Stemmers\n",
    "\n",
    "Suppose you write a stemming scheme and also implement it in a programming language of your choice. How do you then evaluate just how good your stemmer is? That is to say, how do you know how your stemmer fares against any other stemmer? What is needed is a quantifying metric which could assess the goodness of your implementation. There are a few ways one could go about it.\n",
    "\n",
    "### Error rate calculation\n",
    "\n",
    "One way of evaluating stemmers would be to manually construct a dataset consisting of a word and its corresponding stem. For instance, one entry in such a dataset would be *`<walking, walk>`*. *Walking* being the word and *walk* being the desired stem. The evaluation is then done by stemming all the words and checking if the stem produced does indeed match up with the stem provided in the dataset. For example, consider the dataset to have these four entries.\n",
    "\n",
    "1. `<walking, walk>`\n",
    "2. `<walked, walk>`\n",
    "3. `<tasting, tast>`\n",
    "4. `<taste, tast>`\n",
    "\n",
    "When the stemmer from before is run on the dataset, the results obtained would be:\n",
    "\n",
    "1. `<walking, walk>`\n",
    "2. `<walked, walk>`\n",
    "3. `<tasting, tast>`\n",
    "4. `<taste, taste>`\n",
    "\n",
    "Clearly, output of the fourth entry doesn't match up with the one in the dataset. Hence, one word out of four was stemmed incorrectly; the error then would be $$\\frac 1 4 = 25\\%$$\n",
    "\n",
    "This method has the advantage that it is easily implemented. One need only provide the list of manually paired word and its stem. This method, however, is conceptually flawed. The testing is done by comparing the output of the stemmer to the correct stem. *But there is no such thing as a correct stem*. A stemmer can very well reduce the word *walking* to the stem *wal* and it would be correct so long as it also reduces other inflections (*walked* and *walk*) to the same.\n",
    " \n",
    "\n",
    "### Over-Stemming vs Under-Stemming calculation\n",
    "\n",
    "Two very useful metrics to quantify a stemmer with are the over and under stemming errors. Under-stemming is when two related words do not reduce to the same stem. We saw that using the previous stemmer, the word *tasting* conflated to the stem *tast* whereas the word *taste* conflated to the stem *taste*. The two words are related inflections but the stemmer doesn't reduce the two to the same.\n",
    "\n",
    "Similarly, consider the words *red* and *ring*. These two are totally unrelated words. However, using our stemmer, these two are conflated to the same stem i.e. *r*. This is over-stemming i.e. reducing two unrelated words into a same stem.\n",
    "\n",
    "### Paice's Method\n",
    "\n",
    "Counting over-stemming and under-stemming errors seems to be a very useful evaluation scheme for stemmers. After all, this method doesn't need the concept of a correct stem and instead only requires that two related words conflate to a same stem and two unrelated words to different. This is precisely what Paice's method does. To illustrate the concept further, let us first create a toy stemmer."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['tast', 'tast', 'taste']\n"
     ]
    }
   ],
   "source": [
    "class Stemmer:\n",
    "    def __init__(self, suffixes):\n",
    "        assert isinstance(suffixes, list)\n",
    "        self.suffixes = suffixes[:]\n",
    "        \n",
    "    def stem_word(self, word):\n",
    "        for suffix in self.suffixes:\n",
    "            if word.endswith(suffix):\n",
    "                word = word[:-len(suffix)]\n",
    "                break\n",
    "        return word\n",
    "    \n",
    "    def stem_words(self, words):\n",
    "        assert isinstance(words, list)\n",
    "        return map(self.stem_word, words)\n",
    "\n",
    "toy_stemmer = Stemmer([\"s\", \"ing\", \"ed\"])\n",
    "print toy_stemmer.stem_words([\"tasting\", \"tasted\", \"taste\"])\n",
    "# print toy_stemmer.stem_words([\"walk\", \"walked\", \"walking\"])\n",
    "# print toy_stemmer.stem_words([\"red\", \"ring\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It is important to understand that the stemmer above strips one and only one suffix from the given word. So the word _winnings_ will be stemmed to _winning_ i.e. it only strips the suffix _s_ and not the second suffix _ing_. Later, we'll modify this stemmer to repeatedly strip all the suffixes present and compare it to the existing one.\n",
    "\n",
    "Once we have a functional stemmer, to evaluate it we'll need to count the over-stemming and under-stemming errors. But before we do that, we also need to know just which words are meant to conflate to a same stem and which words to different one. In Paice method, this is done by introducing what is known as a *concept group*, which is essentially a list of words that are meant to conflate to the same word stem. For this example, we use following concept groups.\n",
    "\n",
    "1. walk, walks, walked, walking\n",
    "2. taste, tastes, tasted, tasting\n",
    "3. red, reds\n",
    "4. ring, ringing\n",
    "\n",
    "<!--The idea is that whenever you are querying an IR system using a word from one group, the IR should fetch documents that also contains different words from the same group. For instance, suppose you search *George walked* then you might also be interested in documents that contains the text *George was seen walking to his home*. -->\n",
    "\n",
    "We can extend the notion of *concept groups* further by introducing *concept pairs* and *non-concept pairs*. A *concept pair* is a pair of words which should conflate to the same stem. From group (1) above, we can derive following *concept pairs*.\n",
    "\n",
    "* `<walk, walks>`\n",
    "* `<walk, walked>`\n",
    "* `<walk, walking>`\n",
    "* `<walks, walked>`\n",
    "* `<walks, walking>`\n",
    "* `<walked, walking>`\n",
    "\n",
    "Notice that the ordering doesn't really matter here, so even though the pair `<walks, walk>` is a valid _concept pair_, it is not included because `<walk, walks>` is already present in the list.\n",
    "\n",
    "Just how many concept pairs can be obtained from a group with _n_ number of words? With simple combinatorics, the answer is $$n\\choose2$$\n",
    "\n",
    "Again, if we have the number of _concept pairs_ from one group, then the number of _concept pairs_ across all the groups becomes $$\\sum_{g}{n_g\\choose2}$$\n",
    "\n",
    "Where $n_g$ is the number of words in group _g_. This is also known as the _Global Desired Merge Total_ or **GDMT**. This can also be rewritten as:\n",
    "\n",
    "$$GDMT = \\sum_{g}{n_g\\choose2} = \\sum_{g}{\\frac{n_g(n_g-1)}{2}} $$\n",
    "\n",
    "Moving on to the *non-concept pairs*, it is merely the pairs of words that are not meant to conflate together to a same stem. For instance, using group (3) from above, we can derive the following _non-concept pairs_.\n",
    "\n",
    "* `<red, walk>`\n",
    "* `<red, walks>`\n",
    "* `<red, walked>`\n",
    "* `<red, walking>`\n",
    "* `<red, taste>`\n",
    "* `<red, tastes>`\n",
    "* `<red, tasted>`\n",
    "* `<red, tasting>`\n",
    "* `<red, ring>`\n",
    "* `<red, rings>`\n",
    "\n",
    "To get the number of non-concept pairs from one group, consider the following approach. Suppose there are $n_g$ number of words in a group and suppose that the total number of words across all groups is $W$. First off, we choose one word from the said group. This can be done in $n_g$ number of ways. After that, we choose any word from the rest of the group, which can be done in $W-n_g$ ways. Again, since the ordering doesn't really matter, we divide the whole result by 2. Mathematically, this gives the number of *non-concept pairs* from a group as:\n",
    "\n",
    "$$\\frac{n_g(W-n_g)}{2}$$\n",
    "\n",
    "Once gain, the total number of *non-concept pairs* can be found by summing the result above across all *concept groups*. This is also called the *Global Desired Non-Merge Total* or **GDNT**.\n",
    "\n",
    "$$ GDNT = \\sum_{g}{\\frac{n_g(W-n_g)}{2}}$$\n",
    "\n",
    "Let us now calculate the metrics $GDMT$ and $GDNT$ for the concept group described above."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "GDMT = 14\n",
      "GDNT = 52\n"
     ]
    }
   ],
   "source": [
    "concept_groups = [\n",
    "    ['walk', 'walks', 'walked', 'walking'],\n",
    "    ['taste', 'tastes', 'tasted', 'tasting'],\n",
    "    ['red', 'reds'],\n",
    "    ['ring', 'ringing']\n",
    "]\n",
    "\n",
    "def GDMT(cg):\n",
    "    gdmt = 0\n",
    "    for g in cg:\n",
    "        ng = len(g)\n",
    "        gdmt += 0.5 * ng * (ng-1)\n",
    "    return gdmt\n",
    "\n",
    "def GDNT(cg):\n",
    "    gdnt = 0\n",
    "    W = sum([len(g) for g in cg])\n",
    "    for g in cg:\n",
    "        ng = len(g)\n",
    "        gdnt += 0.5 * ng * (W-ng)\n",
    "    return gdnt\n",
    "\n",
    "print \"GDMT = %d\" % GDMT(concept_groups)\n",
    "print \"GDNT = %d\" % GDNT(concept_groups)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "This shows that there are 14 _concept pairs_ i.e. words that should conflate to the same stem and 52 _non-concept pairs_ i.e. words that shouldn't conflate to the same stem.\n",
    "\n",
    "To evaluate the performance of our stemmer, we must count how many of the *concept pairs* were unmerged i.e. did not conflate to the same stem and how many of the *non-concept pairs* were wrongly merged i.e. conflated to the same stem. This is better illustrated with an example.\n",
    "\n",
    "Consider _concept group_ (2) which has the following words:\n",
    "\n",
    "* taste, tastes, tasted, tasting\n",
    "\n",
    "When stemmed, these words give the following stems."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('taste', 'taste'), ('tastes', 'taste'), ('tasted', 'tast'), ('tasting', 'tast')]\n"
     ]
    }
   ],
   "source": [
    "group_2 = concept_groups[1]\n",
    "print zip(group_2, toy_stemmer.stem_words(group_2))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "### Unmerged Concept Pairs and Understemming Error\n",
    "Simple observation tells us that the unmerged _concept pairs_ for this _concept group_ are:\n",
    "\n",
    "* `<taste, tasted>`\n",
    "* `<taste, tasting>`\n",
    "* `<tastes, tasted>`\n",
    "* `<tastes, tasting>`\n",
    "\n",
    "Concretely, _taste_ stemmed to _taste_ while _tasted_ stemmed to _tast_ and so on for the rest of the three pairs. To count these unmerged pairs for a _concept group_, following algorithm could be used:\n",
    "\n",
    "1. Determine all unique stems for a concept group.\n",
    "2. Count the number of words associated with each stem.\n",
    "3. For each unique stem, calculate $0.5 * u_i(n_g-u_i)$. Where $u_i$ is the number of words that reduce to that particular stem and $n_g$ is the number of words in that group.\n",
    "4. Sum up (3) for all unique stems.\n",
    "\n",
    "The concept group above gives two distinct stems for the constituent words. These distinct stems and their instances are:\n",
    "\n",
    "* taste -> 2 times (taste and tastes)\n",
    "* tast  -> 2 times (tasting and tasted)\n",
    "\n",
    "The concept group (3) has 4 words in it. According to the algorithm above, the total unmerged pairs in this group then becomes\n",
    "\n",
    "- For _taste_\n",
    "  - $0.5 * 2 * (4-2) = 2$\n",
    "- For _tasting_\n",
    "  - $0.5 * 2 * (4-2) = 2$\n",
    "\n",
    "i.e. 4 unmerged pairs in total, which is the same as the number of unmerged pairs listed above. When the number of unmerged pairs is calculated for all of the concept groups and summed up, this gives the total _concept pairs_ that were unmerged. This value is called _Global Unachieved Merge Total_ or **GUMT**."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "For group ['walk', 'walks', 'walked', 'walking']\n",
      "Unique stems with their instances are: {'walk': 4}\n",
      "Unmerged concept pairs in this group = 0\n",
      "\n",
      "For group ['taste', 'tastes', 'tasted', 'tasting']\n",
      "Unique stems with their instances are: {'taste': 2, 'tast': 2}\n",
      "Unmerged concept pairs in this group = 4\n",
      "\n",
      "For group ['red', 'reds']\n",
      "Unique stems with their instances are: {'r': 1, 'red': 1}\n",
      "Unmerged concept pairs in this group = 1\n",
      "\n",
      "For group ['ring', 'ringing']\n",
      "Unique stems with their instances are: {'ring': 1, 'r': 1}\n",
      "Unmerged concept pairs in this group = 1\n",
      "\n",
      "GUMT = 6\n"
     ]
    }
   ],
   "source": [
    "from collections import Counter\n",
    "def GUMT(cg, debug=False):\n",
    "    gumt = 0\n",
    "    for g in cg:\n",
    "        stems = toy_stemmer.stem_words(g)\n",
    "        ng = len(g)\n",
    "        unique_stems = Counter(stems)\n",
    "        if debug:\n",
    "            print \"For group \" + repr(g) + \"\\n\" + \"Unique stems with their instances are: \" \\\n",
    "            + repr(dict(unique_stems))\n",
    "        umt = 0\n",
    "        for unique_stem, count in unique_stems.items():\n",
    "            umt += 0.5 * count * (ng-count)\n",
    "        gumt += umt\n",
    "        if debug:\n",
    "            print \"Unmerged concept pairs in this group = %d\\n\" % umt\n",
    "\n",
    "    return gumt\n",
    "\n",
    "print \"GUMT = %d\" % GUMT(concept_groups, True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Once we have the $GUMT$, we can then find out the _understemming error_. This is simply given by the total number of _unmerged concept pairs_ (or $GUMT$) divided by total number of _concept pairs_ (or $GDMT$). This understemming error is also called the _Understemming Index_ or __UI__."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Understemming Error = 42.86%\n"
     ]
    }
   ],
   "source": [
    "def UI(cg):\n",
    "    return GUMT(cg)/GDMT(cg)\n",
    "\n",
    "print \"Understemming Error = %.2f%%\" % (UI(concept_groups)*100)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "### Wrongly merged concept pairs and Overstemming Error\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
